# Search-o1-KG ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
2. [æ•°æ®é›†ä¸‹è½½ä¸å¤„ç†](#æ•°æ®é›†ä¸‹è½½ä¸å¤„ç†)
3. [é¡¹ç›®æ–‡ä»¶ç»“æ„è¯´æ˜](#é¡¹ç›®æ–‡ä»¶ç»“æ„è¯´æ˜)
4. [è¿è¡Œé¡¹ç›®](#è¿è¡Œé¡¹ç›®)
5. [APIå¯†é’¥é…ç½®](#apiå¯†é’¥é…ç½®)
6. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
7. [ç»“æœåˆ†æ](#ç»“æœåˆ†æ)

## ğŸ”§ ç¯å¢ƒé…ç½®

### 1. ç³»ç»Ÿè¦æ±‚
- Python 3.9+
- CUDA 11.0+ (æ¨èä½¿ç”¨GPU)
- è‡³å°‘16GBå†…å­˜
- 50GBå¯ç”¨ç£ç›˜ç©ºé—´

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n search_o1_kg python=3.9
conda activate search_o1_kg

# æˆ–è€…ä½¿ç”¨venv
python -m venv search_o1_kg_env
source search_o1_kg_env/bin/activate  # Linux/Mac
# search_o1_kg_env\Scripts\activate  # Windows
```

### 3. å®‰è£…ä¾èµ–
```bash
cd /home/yy/projects/search-o1-kg

# å®‰è£…åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# ä¸‹è½½spaCyæ¨¡å‹
python -m spacy download en_core_web_sm

# å®‰è£…é¢å¤–çš„ä¾èµ–ï¼ˆå¦‚æœé‡åˆ°é—®é¢˜ï¼‰
pip install --upgrade pip
pip install setuptools wheel
```

### 4. å®‰è£…vLLMï¼ˆç”¨äºé«˜æ•ˆæ¨ç†ï¼‰
```bash
# å®‰è£…vLLMï¼ˆæ¨èGPUç‰ˆæœ¬ï¼‰
pip install vllm

# å¦‚æœCPUç‰ˆæœ¬ï¼Œå¯èƒ½éœ€è¦é¢å¤–é…ç½®
# export VLLM_USE_MODELSCOPE=1
```

## ğŸ“¥ æ•°æ®é›†ä¸‹è½½ä¸å¤„ç†

### 1. æ”¯æŒçš„æ•°æ®é›†

#### ç§‘å­¦æ¨ç†æ•°æ®é›†
- **GPQA** (Graduate-Level Google-Proof Q&A)
- **MATH500** (æ•°å­¦æ¨ç†æ•°æ®é›†)
- **AIME** (American Invitational Mathematics Examination)
- **AMC** (American Mathematics Competitions)

#### å¼€æ”¾åŸŸé—®ç­”æ•°æ®é›†
- **NQ** (Natural Questions)
- **TriviaQA**
- **HotpotQA** (å¤šè·³é—®ç­”)
- **2WikiMultihopQA**
- **MuSiQue**
- **Bamboogle**

#### ä»£ç æ•°æ®é›†
- **LiveCodeBench**

### 2. æ•°æ®é›†ä¸‹è½½æ–¹æ³•

#### æ–¹æ³•1: ä½¿ç”¨Hugging Face Datasetsï¼ˆæ¨èï¼‰

```python
# åˆ›å»ºä¸‹è½½è„šæœ¬ download_datasets.py
from datasets import load_dataset
import json
import os

# ä¸‹è½½æ•°æ®é›†å‡½æ•°
def download_and_save_dataset(dataset_name, split, output_dir):
    os.makedirs(output_dir, exist_ok=True)

    if dataset_name == "gpqa":
        dataset = load_dataset("Idavidrein/gpqa", "diamond")
        data = dataset["train"]
    elif dataset_name == "math500":
        dataset = load_dataset("allenai/MathHub", "MATH500")
        data = dataset["test"]
    elif dataset_name == "nq":
        dataset = load_dataset("nq_open", "validation")
        data = dataset
    elif dataset_name == "triviaqa":
        dataset = load_dataset("trivia_qa", "unfiltered")
        data = dataset["validation"]
    elif dataset_name == "hotpotqa":
        dataset = load_dataset("hotpot_qa", "fullwiki")
        data = dataset["validation"]
    else:
        print(f"Dataset {dataset_name} not yet configured")
        return

    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    processed_data = []
    for item in data:
        processed_item = {
            "Question": item["question"] if "question" in item else item.get("Question", ""),
            "Answer": item.get("answer", item.get("Answer", "")),
            "Correct Choice": item.get("correct_choice", item.get("Correct Choice", ""))
        }
        processed_data.append(processed_item)

    # ä¿å­˜æ–‡ä»¶
    output_file = os.path.join(output_dir, f"{split}.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(processed_data, f, ensure_ascii=False, indent=2)

    print(f"Dataset {dataset_name} saved to {output_file}")

# ä¸‹è½½ç¤ºä¾‹
download_and_save_dataset("gpqa", "diamond", "./data/GPQA")
download_and_save_dataset("math500", "test", "./data/MATH500")
```

#### æ–¹æ³•2: æ‰‹åŠ¨ä¸‹è½½å’Œå¤„ç†

```bash
# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data/{GPQA,MATH500,QA_Datasets,LIVECODEBENCH}

# GPQAæ•°æ®é›†
cd data/GPQA
wget https://huggingface.co/datasets/Idavidrein/gpqa/resolve/main/diamond/train.jsonl
# è½¬æ¢ä¸ºJSONæ ¼å¼

# MATH500æ•°æ®é›†
cd ../MATH500
wget https://huggingface.co/datasets/allenai/MathHub/resolve/main/MATH500/test.jsonl
# è½¬æ¢ä¸ºJSONæ ¼å¼
```

### 3. æ•°æ®æ ¼å¼æ ‡å‡†åŒ–

æ‰€æœ‰æ•°æ®é›†éœ€è¦è½¬æ¢ä¸ºä»¥ä¸‹æ ¼å¼ï¼š

```json
[
  {
    "Question": "é—®é¢˜æ–‡æœ¬",
    "Answer": "ç­”æ¡ˆæ–‡æœ¬",  // å¯é€‰
    "Correct Choice": "æ­£ç¡®é€‰æ‹©"  // å¤šé€‰é¢˜ï¼Œå¯é€‰
  }
]
```

### 4. æ•°æ®é¢„å¤„ç†è„šæœ¬

åˆ›å»º `scripts/preprocess_data.py`:

```python
import json
import os
import re

def standardize_dataset(input_file, output_file, dataset_type="qa"):
    """æ ‡å‡†åŒ–æ•°æ®é›†æ ¼å¼"""

    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    standardized_data = []

    for item in data:
        standardized_item = {
            "Question": item.get("Question", item.get("question", "")),
        }

        # æ ¹æ®æ•°æ®ç±»å‹æ·»åŠ ç›¸åº”å­—æ®µ
        if dataset_type == "multiple_choice":
            standardized_item["Correct Choice"] = item.get("Correct Choice", item.get("correct_choice", ""))
        elif dataset_type == "qa":
            standardized_item["Answer"] = item.get("Answer", item.get("answer", ""))

        # æ¸…ç†é—®é¢˜æ–‡æœ¬
        question = standardized_item["Question"]
        question = re.sub(r'\s+', ' ', question).strip()
        standardized_item["Question"] = question

        standardized_data.append(standardized_item)

    # ä¿å­˜æ ‡å‡†åŒ–æ•°æ®
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(standardized_data, f, ensure_ascii=False, indent=2)

    print(f"Processed {len(standardized_data)} items from {input_file} to {output_file}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®é›†
    standardize_dataset("raw_gpqa.json", "data/GPQA/diamond.json", "multiple_choice")
    standardize_dataset("raw_math500.json", "data/MATH500/test.json", "qa")
    standardize_dataset("raw_nq.json", "data/QA_Datasets/nq.json", "qa")
```

## ğŸ“ é¡¹ç›®æ–‡ä»¶ç»“æ„è¯´æ˜

### æ ¹ç›®å½•æ–‡ä»¶

```
search-o1-kg/
â”œâ”€â”€ README.md                   # é¡¹ç›®ä»‹ç»æ–‡æ¡£
â”œâ”€â”€ USAGE_GUIDE.md             # ä½¿ç”¨æŒ‡å—ï¼ˆæœ¬æ–‡æ¡£ï¼‰
â”œâ”€â”€ PROJECT_SUMMARY.md         # é¡¹ç›®æ€»ç»“
â”œâ”€â”€ requirements.txt           # Pythonä¾èµ–åˆ—è¡¨
â”œâ”€â”€ LICENSE                    # å¼€æºè®¸å¯è¯ï¼ˆMITï¼‰
```

### src/ ç›®å½•ï¼ˆæ ¸å¿ƒæ¨¡å—ï¼‰

```
src/
â”œâ”€â”€ __init__.py                # åŒ…åˆå§‹åŒ–æ–‡ä»¶ï¼Œå¯¼å‡ºä¸»è¦ç±»
â”œâ”€â”€ knowledge_graph/           # çŸ¥è¯†å›¾è°±æ„å»ºæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py           # å¯¼å‡ºå›¾è°±ç›¸å…³ç±»
â”‚   â”œâ”€â”€ entity_extractor.py   # å®ä½“æŠ½å–å™¨ï¼ˆæ ¸å¿ƒæ–‡ä»¶ï¼‰
â”‚   â”œâ”€â”€ relation_extractor.py # å…³ç³»æŠ½å–å™¨
â”‚   â”œâ”€â”€ graph_builder.py      # å›¾è°±æ„å»ºå™¨
â”‚   â””â”€â”€ graph_storage.py      # å›¾è°±å­˜å‚¨ç³»ç»Ÿ
â”œâ”€â”€ gnn_reasoning/             # å›¾ç¥ç»ç½‘ç»œæ¨ç†æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py           # å¯¼å‡ºæ¨ç†ç›¸å…³ç±»
â”‚   â”œâ”€â”€ graph_neural_network.py # GNNå®ç°ï¼ˆGCN/GAT/GraphSAGEï¼‰
â”‚   â”œâ”€â”€ reasoning_engine.py   # æ¨ç†å¼•æ“ï¼ˆæ ¸å¿ƒæ–‡ä»¶ï¼‰
â”‚   â”œâ”€â”€ path_finder.py        # è·¯å¾„æŸ¥æ‰¾å™¨
â”‚   â””â”€â”€ entity_linker.py      # å®ä½“é“¾æ¥å™¨
â””â”€â”€ multimodal_alignment/      # è·¨æ¨¡æ€å¯¹é½æ¨¡å—
    â”œâ”€â”€ __init__.py           # å¯¼å‡ºå¯¹é½ç›¸å…³ç±»
    â”œâ”€â”€ multimodal_aligner.py # å¯¹é½å™¨ï¼ˆæ ¸å¿ƒæ–‡ä»¶ï¼‰
    â”œâ”€â”€ visualization_engine.py # å¯è§†åŒ–å¼•æ“
    â””â”€â”€ alignment_utils.py    # å¯¹é½å·¥å…·
```

### scripts/ ç›®å½•

```
scripts/
â”œâ”€â”€ run_search_o1_kg.py       # ä¸»æ¨ç†è„šæœ¬ï¼ˆæœ€é‡è¦çš„æ–‡ä»¶ï¼‰
â”œâ”€â”€ download_datasets.py      # æ•°æ®é›†ä¸‹è½½è„šæœ¬
â”œâ”€â”€ preprocess_data.py        # æ•°æ®é¢„å¤„ç†è„šæœ¬
â”œâ”€â”€ evaluate_kg.py           # çŸ¥è¯†å›¾è°±è¯„ä¼°è„šæœ¬
â””â”€â”€ visualize_reasoning.py   # æ¨ç†å¯è§†åŒ–è„šæœ¬
```

### å…¶ä»–ç›®å½•

```
data/                        # æ•°æ®ç›®å½•
â”œâ”€â”€ GPQA/                   # GPQAæ•°æ®é›†
â”œâ”€â”€ MATH500/                # MATH500æ•°æ®é›†
â”œâ”€â”€ QA_Datasets/            # é—®ç­”æ•°æ®é›†
â””â”€â”€ LIVECODEBENCH/          # ä»£ç æ•°æ®é›†

cache/                      # ç¼“å­˜ç›®å½•
â”œâ”€â”€ search_cache.json       # æœç´¢ç»“æœç¼“å­˜
â”œâ”€â”€ url_cache.json         # URLå†…å®¹ç¼“å­˜
â””â”€â”€ kg_cache/              # çŸ¥è¯†å›¾è°±ç¼“å­˜

outputs/                    # è¾“å‡ºç›®å½•
â”œâ”€â”€ runs.baselines/        # åŸºçº¿æ¨¡å‹ç»“æœ
â”œâ”€â”€ runs.qa/              # é—®ç­”ä»»åŠ¡ç»“æœ
â””â”€â”€ runs.analysis/        # åˆ†æç»“æœ

tests/                      # æµ‹è¯•ç›®å½•
â”œâ”€â”€ test_kg.py            # çŸ¥è¯†å›¾è°±æµ‹è¯•
â”œâ”€â”€ test_gnn.py           # GNNæµ‹è¯•
â””â”€â”€ test_alignment.py     # å¯¹é½æµ‹è¯•

docs/                       # æ–‡æ¡£ç›®å½•
â”œâ”€â”€ api_reference.md      # APIå‚è€ƒæ–‡æ¡£
â””â”€â”€ examples.md           # ä½¿ç”¨ç¤ºä¾‹
```

## ğŸš€ è¿è¡Œé¡¹ç›®

### 1. å‡†å¤‡APIå¯†é’¥

#### Bing Search API
```bash
# 1. è®¿é—® https://azure.microsoft.com/en-us/services/cognitive-services/bing-web-search-api/
# 2. åˆ›å»ºAzureè´¦æˆ·å¹¶è®¢é˜…Bing Search API
# 3. è·å–è®¢é˜…å¯†é’¥
export BING_SUBSCRIPTION_KEY="your_bing_subscription_key_here"
```

#### Jina APIï¼ˆå¯é€‰ï¼‰
```bash
# 1. è®¿é—® https://jina.ai/reader/
# 2. æ³¨å†Œå¹¶è·å–APIå¯†é’¥
export JINA_API_KEY="your_jina_api_key_here"
```

### 2. å‡†å¤‡æ¨¡å‹

#### ä½¿ç”¨Hugging Faceæ¨¡å‹
```bash
# æ¨èçš„æ¨¡å‹é€‰æ‹©
export MODEL_PATH="microsoft/DialoGPT-medium"  # ç¤ºä¾‹
# æˆ–ä½¿ç”¨å…¶ä»–å¤§æ¨¡å‹
export MODEL_PATH="meta-llama/Llama-2-7b-chat-hf"
export MODEL_PATH="Qwen/Qwen-7B-Chat"
```

### 3. åŸºæœ¬è¿è¡Œå‘½ä»¤

```bash
# åŸºæœ¬æ¨ç†å‘½ä»¤
cd /home/yy/projects/search-o1-kg

python scripts/run_search_o1_kg.py \
    --dataset_name gpqa \
    --split diamond \
    --model_path "your_model_path" \
    --bing_subscription_key "your_bing_key" \
    --jina_api_key "your_jina_key" \
    --subset_num 10  # å…ˆæµ‹è¯•10ä¸ªæ ·æœ¬
```

### 4. è¯¦ç»†å‚æ•°è¯´æ˜

#### æ ¸å¿ƒå‚æ•°
```bash
--dataset_name          # æ•°æ®é›†åç§° [gpqa, math500, aime, nq, hotpotqaç­‰]
--split                 # æ•°æ®åˆ†å‰² [test, diamond, train, validation]
--model_path           # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
--subset_num           # å¤„ç†æ ·æœ¬æ•°é‡ï¼ˆ-1è¡¨ç¤ºå…¨éƒ¨ï¼‰
```

#### æœç´¢å‚æ•°
```bash
--max_search_limit     # æœ€å¤§æœç´¢æ¬¡æ•° [é»˜è®¤: 10]
--max_turn             # æœ€å¤§æ¨ç†è½®æ¬¡ [é»˜è®¤: 15]
--top_k                # è¿”å›æ–‡æ¡£æ•°é‡ [é»˜è®¤: 10]
--max_doc_len          # æ–‡æ¡£æœ€å¤§é•¿åº¦ [é»˜è®¤: 3000]
--use_jina             # æ˜¯å¦ä½¿ç”¨Jina API [é»˜è®¤: True]
```

#### GNNå‚æ•°
```bash
--gnn_hidden_dim       # GNNéšè—å±‚ç»´åº¦ [é»˜è®¤: 128]
--gnn_output_dim       # GNNè¾“å‡ºç»´åº¦ [é»˜è®¤: 64]
--gnn_num_layers       # GNNå±‚æ•° [é»˜è®¤: 3]
--gnn_dropout          # Dropoutç‡ [é»˜è®¤: 0.1]
```

#### é‡‡æ ·å‚æ•°
```bash
--temperature          # é‡‡æ ·æ¸©åº¦ [é»˜è®¤: 0.7]
--top_p               # Top-pé‡‡æ · [é»˜è®¤: 0.8]
--top_k_sampling      # Top-ké‡‡æ · [é»˜è®¤: 20]
--repetition_penalty  # é‡å¤æƒ©ç½š [é»˜è®¤: 1.05]
```

### 5. ä¸åŒæ•°æ®é›†çš„è¿è¡Œç¤ºä¾‹

#### GPQAï¼ˆç§‘å­¦é—®ç­”ï¼‰
```bash
python scripts/run_search_o1_kg.py \
    --dataset_name gpqa \
    --split diamond \
    --model_path "microsoft/DialoGPT-medium" \
    --bing_subscription_key "$BING_SUBSCRIPTION_KEY" \
    --jina_api_key "$JINA_API_KEY" \
    --max_search_limit 5 \
    --max_turn 15 \
    --subset_num 50
```

#### MATH500ï¼ˆæ•°å­¦æ¨ç†ï¼‰
```bash
python scripts/run_search_o1_kg.py \
    --dataset_name math500 \
    --split test \
    --model_path "microsoft/DialoGPT-medium" \
    --bing_subscription_key "$BING_SUBSCRIPTION_KEY" \
    --jina_api_key "$JINA_API_KEY" \
    --max_search_limit 3 \
    --max_turn 10 \
    --subset_num 30
```

#### HotpotQAï¼ˆå¤šè·³é—®ç­”ï¼‰
```bash
python scripts/run_search_o1_kg.py \
    --dataset_name hotpotqa \
    --split validation \
    --model_path "microsoft/DialoGPT-medium" \
    --bing_subscription_key "$BING_SUBSCRIPTION_KEY" \
    --jina_api_key "$JINA_API_KEY" \
    --max_search_limit 10 \
    --max_turn 15 \
    --subset_num 20
```

## ğŸ”‘ APIå¯†é’¥é…ç½®

### 1. ç¯å¢ƒå˜é‡æ–¹å¼ï¼ˆæ¨èï¼‰

åˆ›å»º `.env` æ–‡ä»¶ï¼š
```bash
# .env æ–‡ä»¶å†…å®¹
BING_SUBSCRIPTION_KEY=your_actual_bing_key_here
JINA_API_KEY=your_actual_jina_key_here
HUGGINGFACE_TOKEN=your_huggingface_token_here  # å¦‚æœéœ€è¦ç§æœ‰æ¨¡å‹
```

åŠ è½½ç¯å¢ƒå˜é‡ï¼š
```bash
# å®‰è£…python-dotenv
pip install python-dotenv

# åœ¨è„šæœ¬å¼€å§‹å¤„æ·»åŠ 
from dotenv import load_dotenv
load_dotenv()
```

### 2. å‘½ä»¤è¡Œå‚æ•°æ–¹å¼

```bash
# ç›´æ¥åœ¨å‘½ä»¤è¡Œä¸­ä¼ é€’
python scripts/run_search_o1_kg.py \
    --dataset_name gpqa \
    --model_path "model_path" \
    --bing_subscription_key "your_key" \
    --jina_api_key "your_key"
```

### 3. é…ç½®æ–‡ä»¶æ–¹å¼

åˆ›å»º `config/config.json`:
```json
{
  "api_keys": {
    "bing_subscription_key": "your_key",
    "jina_api_key": "your_key"
  },
  "model_paths": {
    "default": "microsoft/DialoGPT-medium",
    "math": "microsoft/DialoGPT-medium"
  },
  "gnn_config": {
    "hidden_dim": 128,
    "output_dim": 64,
    "num_layers": 3,
    "dropout": 0.1
  }
}
```

## ğŸ”§ æ•…éšœæ’é™¤

### 1. å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

#### CUDAå†…å­˜ä¸è¶³
```bash
# é”™è¯¯: CUDA out of memory
# è§£å†³æ–¹æ¡ˆ:
python scripts/run_search_o1_kg.py \
    --subset_num 1 \  # å‡å°‘æ ·æœ¬æ•°é‡
    --gnn_hidden_dim 64 \  # å‡å°‘GNNç»´åº¦
    --max_doc_len 1000  # å‡å°‘æ–‡æ¡£é•¿åº¦
```

#### æ¨¡å‹åŠ è½½å¤±è´¥
```bash
# é”™è¯¯: model not found
# è§£å†³æ–¹æ¡ˆ:
# 1. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®
# 2. ç¡®ä¿æœ‰ç½‘ç»œè¿æ¥è®¿é—®Hugging Face
# 3. ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
export MODEL_PATH="/path/to/your/local/model"
```

#### APIè°ƒç”¨å¤±è´¥
```bash
# é”™è¯¯: API key invalid
# è§£å†³æ–¹æ¡ˆ:
# 1. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®
# 2. ç¡®è®¤APIé…é¢æ˜¯å¦å……è¶³
# 3. æ£€æŸ¥ç½‘ç»œè¿æ¥
```

#### ä¾èµ–åŒ…å†²çª
```bash
# é”™è¯¯: package version conflict
# è§£å†³æ–¹æ¡ˆ:
pip install --upgrade pip
pip install --force-reinstall -r requirements.txt
```

### 2. è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export PYTHONPATH=/home/yy/projects/search-o1-kg/src:$PYTHONPATH
python -u scripts/run_search_o1_kg.py --dataset_name gpqa --subset_num 1 --model_path "model_path" --bing_subscription_key "key" 2>&1 | tee debug.log
```

### 3. æ€§èƒ½ç›‘æ§

```bash
# ç›‘æ§GPUä½¿ç”¨
watch -n 1 nvidia-smi

# ç›‘æ§å†…å­˜ä½¿ç”¨
htop

# ç›‘æ§è¿›ç¨‹
ps aux | grep python
```

## ğŸ“Š ç»“æœåˆ†æ

### 1. è¾“å‡ºæ–‡ä»¶è¯´æ˜

è¿è¡Œå®Œæˆåï¼Œä¼šåœ¨ `outputs/` ç›®å½•ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

```
outputs/
â”œâ”€â”€ dataset_name.model_name.search_o1_kg/
â”‚   â”œâ”€â”€ test.info_extract.json           # è¯¦ç»†æ¨ç†è®°å½•
â”‚   â”œâ”€â”€ test.output.json                 # æ¨¡å‹è¾“å‡º
â”‚   â””â”€â”€ test.metrics.json               # è¯„ä¼°æŒ‡æ ‡
```

### 2. æŸ¥çœ‹æ¨ç†ç»“æœ

```python
# åˆ›å»ºåˆ†æè„šæœ¬ analyze_results.py
import json

def analyze_results(output_file):
    with open(output_file, 'r') as f:
        results = json.load(f)

    print(f"æ€»å…±å¤„ç†äº† {len(results)} ä¸ªæ ·æœ¬")

    for i, result in enumerate(results[:5]):  # æŸ¥çœ‹å‰5ä¸ªç»“æœ
        print(f"\næ ·æœ¬ {i+1}:")
        print(f"è¾“å…¥é•¿åº¦: {len(result['prompt'])}")
        print(f"è¾“å‡ºé•¿åº¦: {len(result.get('raw_output', ''))}")
        print(f"æ˜¯å¦ä½¿ç”¨KGå¢å¼º: {result.get('kg_enhanced', False)}")
        if 'extracted_info' in result:
            print(f"æå–ä¿¡æ¯é•¿åº¦: {len(result['extracted_info'])}")

# ä½¿ç”¨ç¤ºä¾‹
analyze_results("outputs/gpqa.model_name.search_o1_kg/test.info_extract.json")
```

### 3. çŸ¥è¯†å›¾è°±åˆ†æ

```python
# åˆ›å»ºKGåˆ†æè„šæœ¬ analyze_kg.py
import sys
sys.path.append('/home/yy/projects/search-o1-kg/src')
from knowledge_graph import KnowledgeGraphBuilder

def analyze_knowledge_graph(documents):
    builder = KnowledgeGraphBuilder()
    kg = builder.build_graph_from_documents(documents)

    print("çŸ¥è¯†å›¾è°±ç»Ÿè®¡:")
    print(f"å®ä½“æ•°é‡: {len(kg.entities)}")
    print(f"å…³ç³»æ•°é‡: {len(kg.relations)}")
    print(f"å›¾èŠ‚ç‚¹æ•°: {kg.graph.number_of_nodes()}")
    print(f"å›¾è¾¹æ•°: {kg.graph.number_of_edges()}")

    # å®ä½“ç±»å‹åˆ†å¸ƒ
    entity_types = {}
    for entity in kg.entities.values():
        entity_types[entity.entity_type] = entity_types.get(entity.entity_type, 0) + 1

    print("\nå®ä½“ç±»å‹åˆ†å¸ƒ:")
    for entity_type, count in entity_types.items():
        print(f"  {entity_type}: {count}")

    return kg

# ä½¿ç”¨ç¤ºä¾‹
documents = ["Einstein worked at Princeton University.", "E=mcÂ² is his famous equation."]
kg = analyze_knowledge_graph(documents)
```

### 4. æ€§èƒ½å¯¹æ¯”

åˆ›å»ºæ€§èƒ½å¯¹æ¯”è„šæœ¬ï¼š

```python
# performance_comparison.py
import matplotlib.pyplot as plt
import json

def compare_performance():
    # å‡è®¾çš„ç»“æœæ•°æ®
    models = ['Search-o1', 'Search-o1-KG']
    datasets = ['GPQA', 'MATH500', 'HotpotQA']

    # ç¤ºä¾‹æ•°æ®ï¼ˆéœ€è¦æ›¿æ¢ä¸ºå®é™…ç»“æœï¼‰
    results = {
        'GPQA': [58.2, 62.1],
        'MATH500': [65.4, 68.7],
        'HotpotQA': [72.1, 75.8]
    }

    # ç»˜åˆ¶å¯¹æ¯”å›¾
    fig, ax = plt.subplots(figsize=(10, 6))
    x = range(len(models))

    for i, dataset in enumerate(datasets):
        offset = i * 0.25
        ax.bar([xi + offset for xi in x], results[dataset], width=0.25,
               label=dataset, alpha=0.8)

    ax.set_xlabel('æ¨¡å‹')
    ax.set_ylabel('å‡†ç¡®ç‡ (%)')
    ax.set_title('Search-o1 vs Search-o1-KG æ€§èƒ½å¯¹æ¯”')
    ax.set_xticks([xi + 0.25 for xi in x])
    ax.set_xticklabels(models)
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('outputs/performance_comparison.png')
    plt.show()

if __name__ == "__main__":
    compare_performance()
```

## ğŸ“ å¿«é€Ÿå¼€å§‹æ¸…å•

è¿è¡Œé¡¹ç›®å‰çš„æ£€æŸ¥æ¸…å•ï¼š

- [ ] Python 3.9+ ç¯å¢ƒå·²åˆ›å»º
- [ ] æ‰€æœ‰ä¾èµ–å·²å®‰è£… (`pip install -r requirements.txt`)
- [ ] spaCyæ¨¡å‹å·²ä¸‹è½½ (`python -m spacy download en_core_web_sm`)
- [ ] æ•°æ®é›†å·²ä¸‹è½½å¹¶å¤„ç†ä¸ºæ­£ç¡®æ ¼å¼
- [ ] APIå¯†é’¥å·²é…ç½®
- [ ] æ¨¡å‹è·¯å¾„å·²è®¾ç½®
- [ ] ç¼“å­˜ç›®å½•å·²åˆ›å»º (`mkdir -p cache outputs`)

å®Œæˆä»¥ä¸Šæ­¥éª¤åï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤æµ‹è¯•ï¼š

```bash
cd /home/yy/projects/search-o1-kg
python scripts/run_search_o1_kg.py \
    --dataset_name gpqa \
    --split diamond \
    --subset_num 1 \
    --model_path "microsoft/DialoGPT-medium" \
    --bing_subscription_key "your_key"
```

å¦‚æœè¿è¡ŒæˆåŠŸï¼Œä½ åº”è¯¥çœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼š
```
å¼€å§‹Search-o1-KGæ¨ç†ï¼Œæ•°æ®é›†: gpqa
åŠ è½½äº† X æ¡æ•°æ®
åˆå§‹åŒ–è¯­è¨€æ¨¡å‹...
åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç»„ä»¶...
-------------- Turn 1 --------------
We have 1 sequences needing generation...
Generation completed, processing outputs...
Batch processing 1 sequences with enhanced KG reasoning...
Batch generation completed, assigning outputs to sequences...
Search-o1-KG inference completed!
```

## ğŸ†˜ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ï¼š

1. æŸ¥çœ‹è¯¦ç»†æ—¥å¿—æ–‡ä»¶
2. è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯å®‰è£…
3. æ£€æŸ¥GitHub Issues
4. è”ç³»é¡¹ç›®ç»´æŠ¤è€…

---

**ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥é˜…æ–‡æ¡£æˆ–æäº¤Issueã€‚**